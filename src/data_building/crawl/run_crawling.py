import json
from src.data_building.config import CATEGORY_IDS, MAX_PER_CATEGORY, OUTPUT_FILE_NAME, CRAWL_PRODUCT_PAGES, CRAWL_REVIEW_PAGES_PER_PRODUCT
from src.data_building.crawl.tiki_crawler import get_products_in_category, get_reviews_for_product

def main_crawl():
    all_comments = 0
    max_total = MAX_PER_CATEGORY * len(CATEGORY_IDS)

    with open(OUTPUT_FILE_NAME, mode="w", encoding="utf-8") as file:
        for cat_id in CATEGORY_IDS:
            print(f"â–¶ï¸ Báº¯t Ä‘áº§u crawl danh má»¥c {cat_id}")
            category_comments = 0
            products = get_products_in_category(cat_id, CRAWL_PRODUCT_PAGES)

            for product in products:
                if category_comments >= MAX_PER_CATEGORY or all_comments >= max_total:
                    break

                spid = product.get('spid')
                product_id = product.get('id')

                if not spid or not product_id:
                    continue

                reviews = get_reviews_for_product(spid, product_id, CRAWL_REVIEW_PAGES_PER_PRODUCT)

                for review in reviews:
                    if category_comments >= MAX_PER_CATEGORY or all_comments >= max_total:
                        break
                    json.dump(review, file, ensure_ascii=False)
                    file.write('\n')

                    category_comments += 1
                    all_comments += 1

            print(f"âœ… HoÃ n táº¥t danh má»¥c {cat_id}: {category_comments} bÃ¬nh luáº­n\n")

    print(f"ğŸ‰ ÄÃ£ crawl tá»•ng cá»™ng {all_comments} bÃ¬nh luáº­n tá»« {len(CATEGORY_IDS)} danh má»¥c!")

if __name__ == "__main__":
    main_crawl()